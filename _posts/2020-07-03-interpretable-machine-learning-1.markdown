---
layout: post
title: "可解释性的机器学习笔记 (I)"
subtitle: '动机，分类和范围'
author: "Xiaozhe Yao"
mathjax: true
header-img: "img/posts/cnn.jpg"
tags:
    - Machine Learning
    - Interpretability
---

可解释性是人们能够理解决策原因的程度。也是指人们能够一致地预测模型结果的程度。

以下原因推动了对可解释性(Interpretability) 和解释 (Explanations) 的需求 (Doshi-Velez 和 Kim (2017),Miller (2017)): 
* **人类的好奇心和学习能力**,这是指人类有着对周围环境的心理模型 (Mental Model),当有意外的事情发生时,人类就会更新这个心理模型,通过为这样的意外事件找到个解释来更新模型。例如,一个人突然感到不舒服,就问自己:“为什么我感到如此不舒服?”。他得出每次吃那些红色浆果后他都会生病,于是他更新了他的心理模型,认为浆果导致了疾病,因此应该避免食用。在研究中使用不透明的机器学习模型时,如果模型仅给出预测而没有解释,则科学发现仍是完全不可知的。为了促进学习并满足人们的好奇心——为什么机器产生了某些预测或行为,那么此时可解释性和解释至关重要。当然,人们不需要对所有发生的一切都进行解释。对于大多数人来说,他们可以不理解计算机是如何工作的。但意外事件使我们好奇, 例如说,“为什么我的计算机意外关闭?”
* 与学习密切相关的是**人类渴望找到事物存在的意义**。我们希望协调我们知识结构要素之间的矛盾或不一致,例如,“为什么我的狗会咬我,尽管它以前从来没有这样做过。” 狗过去的行为与新发生的、令人不快的咬伤经历之间存在着矛盾。兽医的解释调和了狗主人的矛盾:“那只狗因为紧张而咬人。” 机器的决策对人的生活影响越大,机器对它行为的解释就越重要。如果机器学习模型拒绝贷款申请,这对于申请者来说可能是完全意外的。他们只能用某种说法来调和期望和现实之间的这种不一致。这些解释实际上并不需要完全说清楚情况,但应该至少提供一个主因。另一个例子是算法产品推荐,就我个人而言,我一直在思考为什么某些产品或电影被算法推荐给我。通常情况是:网络上的广告跟踪着我,因为我最近买了一台洗衣机, 我知道在接下来的几天里, 洗衣机广告会推送给我; 由于我们的购物车中已经有冬帽, 建议戴手套是很合理的;算法推荐某部电影,是因为和我喜欢相同的其他电影的用户也喜欢着这部电影。互联网公司越来越多地在推荐中添加解释。亚马逊产品推荐就是一个很好的例子, 它基于经常购买的产品组合。
* 在许多科学学科中,从定性方法到定量方法 (例如社会学、心理学),以及到机器学习 (生物学、基因组学) 都发生了变化。科学的目标是获取知识,但是许多问题都是通过大数据集和黑盒机器学习模型来解决的。模型本身应该成为知识的来源,而不是数据。**可解释性使得可以提取模型捕获的这些额外知识**。
* 机器学习模型**承担需要安全措施和测试的实际任务**。想象一下,一辆自动驾驶汽车根据深度学习系统自动检测骑自行车的人。你希望 100% 确定系统所学到的抽象是无错误的,因为要是汽车直接碾过骑车人,这种情况是非常糟糕的。一种解释可能会指出,最重要的学习特征是识别自行车的两个轮子,这种解释可以帮助你思考极端情况,例如自行车的侧袋部分挡住了车轮。
* 默认情况下,机器学习模型从训练数据中学习到了某种偏见,这可能把你的机器学习模型变成歧视受保护群体的种族主义者。**可解释性是机器学习模型中一种有效检测偏见的调试工具**。
训练好的自动批准或拒绝信用申请的机器学习模型可能会歧视少数人。而你的主要目标是只向最终会偿还贷款的人提供贷款。在这种情况下,问题表述的不完整性在于,你不仅想要最大程度地减少贷款违约,而且也有义务不根据某些人又统计数据加以区分。这是问题制定中的一个附加约束 (以低风险和合规的方式发放贷款),而机器学习模型优化所针对的损失函数并没有包含这一约束。
* 将机器和算法整合到日常生活中的过程需要可解释性,以**增加社会认可度**。人们把信仰、欲望、意图等等归因到物体上。在一个著名的实验中,Heider 和 Simmel (1944)[6] 向参与者展示了一些形状的视频, 其中一个圆圈打开一扇 “门” 进入一个 “房间” (这只是一个长方形)。 参与者描述了形状的动作,就像描述人的行为一样,为形状分配了意图、甚至情感和性格特征。机器人就是一个很好的例子,比如说我的一个吸尘器,名字就叫 “Doge”。如果 Doge 被卡住了,我可以这么想:“Doge 想继续打扫,但它因为卡住了而向我求助。” 后来,当 Doge 打扫完并寻找插座充电时, 我想: “Doge 想充电, 并打算找到插座。 ” 我还可以给它赋予性格特征: “Doge 有点笨,但很可爱。” 这些是我的想法,尤其是当我发现 Doge 在尽职地打扫房子的时候撞倒了一棵植物时。能解释其预测的机器或算法会得到更多的认可,同时在后面,我们还会讨论解释是一个社会化过程。
* **解释用于管理社交互动**。通过创造某个事物的共同含义,解释者影响着解释的接收者的行为、情感和信念。对于一个要与我们互动的机器,它可能需要塑造我们的情感和信念。机器必须“说服” 我们,这样它们才能达到预期的目标。如果我的扫地机器人没有在某种程度上解释它的行为,我不会完全接受它。扫地机器人创造了一个共同的含义,例如,对于 “事故” (如再次被困在浴室地毯上),解释说它被卡住了,而不是简单地停止工作却不发表评论。有趣的是, 解释机器的目标 (构建信任) 和接收者的目标 (理解预测或行为) 之间可能存在偏差。也许对Doge 被卡住的原因的完整解释可能是电池电量非常低,其中一个轮子工作不正常,还有一个bug 让机器人即使有障碍物也一次又一次地走到同一个地方。这些原因 (还有其他一些) 导致机器人被卡住, 但它只解释了有什么东西挡在路上, 这足以让我相信它的行为, 并得到事故的共同含义。 顺便说一下, Doge 又被困在浴室里了。 我们每次都要把地毯拆下来, 然后让 Doge 清理。
* **机器学习模型只有在可以解释时才能进行调试和审核**。即使在低风险环境中,例如电影推荐, 在研究和开发阶段以及部署之后, 解释能力也是很有价值的。 之后, 当模型用于产品时, 可能会出错。对错误预测的解释有助于理解错误的原因,它为如何修复系统提供了指导方向。考虑一个哈士奇与狼分类器的例子,分类器将一些哈士奇误分类为狼。使用可解释的机器学习方法,你会发现错误分类是由于图像上的雪造成的。分类器学会了使用雪作为一个特征来将图像分类为狼,这对于在训练数据集中分离狼和哈士奇可能是有道理的,但在实际使用中则不然。


_如果能够确保机器学习模型能够解释决策，我们还可以检查以下性质：_
* 公平性 (Fairness)。确保预测是公正的,不会隐式或显式地歧视受保护的群体。可解释的模型可以告诉你为什么它决定某个人不应该得到贷款,并且使人们更容易判断该决策是否基于学习人又统计学偏见 (例如种族)。
* 隐私 (Privacy)。确保保护数据中的敏感信息。
* 可靠性 (Reliability) 和鲁棒性 (Robustness)。确保输入的小变化不会导致预测发生剧烈变化。
* 因果性 (Causality)。检查是否只找到了因果关系。
* 信任 (Trustness)。与黑箱模型相比，人们更容易信任解释其决策的系统。

_何时不需要可解释性：_
* 如果模型没有重大影响,则不需要解释性。想象一下,一个名为 Mike 的人正在做一个机器学习方面的项目,根据 Facebook 的数据预测他的朋友们下一个假期会去哪里。Mike 就是喜欢有依据地推测朋友们会去哪里度假,从而让他的朋友们吃惊。如果模型是错误的也没有问题(最坏的情况是,Mike 有点尴尬);如果 Mike 不能解释模型的输出,那也没有问题。在这种情况下,没有可解释性是完全可以的。如果 Mike 开始围绕这些度假目的地的预测建立业务, 情况将会改变。如果模型是错误的,企业可能会赔钱,或者模型可能会因为种族偏见而对某些人变得更糟。一旦模型产生重大影响,无论是金融还是社会,可解释性就变得很重要了。
* 当问题被研究得很深入时,就不需要解释性了。一些应用已经得到了充分的研究,因此有足够的模型实践经验,随着时间的推移,模型的问题已经得到解决。一个很好的例子是光学字符识别的机器学习模型,它处理信封中的图像并提取地址。这些系统有多年的使用经验,很明显它们是有效的。此外,我们对获取有关这上面的任务的更多信息并不真正感兴趣。
* 可解释性可能使人或程序能够操纵系统。欺骗系统的用户问题是由模型的创建者和用户的目标不匹配造成的。信用评分就是这样一个系统,因为银行希望确保贷款只发放给可能归还贷款的申请人, 而申请人的目标是获得贷款, 即使银行不想提供给他们。 这两个目标之间的不匹配鼓励申请者对系统进行博弈,以增加他们获得贷款的机会。如果申请人知道拥有两张以上的信用卡会对他的分数产生负面影响,他只需退掉第三张信用卡来提高分数,并在贷款获得批准后申请新的信用卡。虽然他的分数有所提高,但偿还贷款的实际可能性并没有改变。只有当输入是因果特征的代理,而不是实际导致结果时,系统才能被博弈。尽可能避免使用代理特征,因为它们使模型有可能被博弈。例如,Google 开发了一个名为 “Google 流感趋势”的系统来预测流感爆发。该系统将 Google 搜索与流感爆发相关联,但其表现不佳。搜索查询的分布发生了变化,Google 流感趋势错过了许多次流感爆发。Google 搜索不会导致流感,当人们搜索 “发烧” 这样的症状时,这仅仅是与实际流感爆发的关联。理想情况下,模型只使用因果特征,因为它们不可博弈。

_可解释性方法的分类：_

* 本质的(Intrinsic)/事后的(Post-hoc)。本质的可解释性是指由于结构简单而被认为是可解释的机器学习模型,如短的决策树或稀疏线性模型;事后解释性是指模型训练后运用解释方法,例如,置换特征重要性是一种事后解释方法。事后方法也可以应用于本质上是可解释的模型上。例如,可以计算决策树的置换特征重要性。
* 根据解释方法的输出：
  * 特征概要统计量 (Feature Summary Statistic):许多解释方法为每个特征提供概要统计量。
有些方法为每个特征返回一个数字,例如特征重要性,或者更复杂的输出,例如成对特征交互强度,每个特征对表示为一个数字。
  * 特征概要可视化 (Feature Summary Visualization):大多数特征概要统计信息也可以可视化。有些特征概要实际上只有在可视化的情况下才有意义,并且表格不能满足要求。特征的部分依赖就是这样一种情况。部分依赖图是显示特征和平均预测结果的曲线。呈现部分依赖关系的最佳方法是实际绘制曲线,而不是打印坐标。
  * 模型内部 (例如学习的权重) (Model Internals):对于本质上可解释的模型的解释属于这一类,如线性模型中的权重或决策树的学习树结构 (用于分割的特征和阈值)。但对于像线性模型,因为权重同时是模型内部和特征概要统计量,所以此时两者的界限是模糊的。输出模型内部结构的另一种方法是在卷积神经网络中将学习的特征检测器可视化。根据定义,输出模型内部的可解释性方法是特定于模型的 (请参阅下一个标准)。
  * 数据点 (Data Point):这个类别的方法是返回数据点 (已经存在的或新创建的) 以使模型可解释。一种方法叫做反事实解释 (Counterfactual Explanations),为了解释对数据实例的预测,该方法通过用一些方式改变某些特征以改变预测结果 (例如预测类别的翻转),找到相似的数据点。另一个方法是识别预测类的原型,这里输出新数据点的解释方法要求可以解释数据点本身。这对图像和文本很有效,但对于具有数百个特征的表格数据不太有用。
  * 本质上可解释模型:解释黑盒模型的一个解决方案是用可解释模型 (全局地或局部地) 对其进行近似。而这些可解释模型本身可以通过查看模型内部参数或特征概要统计量来解释。
* 模型特定还是模型无关。特定于模型的解释方法仅限于特定的模型类,例如线性模型中回归权重的解释就是特定于模型的解释,因为根据定义,本质上可解释模型的解释通常是特定于模型的解释。仅应用于解释如神经网络的工具也是特定于模型的。相对应的,与模型无关的工具可以用于任何机器学习模型,并在模型经过训练后应用 (事后的)。这些模型无关的方法通常通过分析特征输入和输出来工作。根据定义,这些方法是不能访问模型的内部信息,如权重或结构信息。
* 局部还是全局。解释方法是否解释单个实例预测或整个模型行为?还是范围介于两者之间?